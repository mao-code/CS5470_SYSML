{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "ce9295b2-182b-490b-8325-83a67c4a001d",
      "metadata": {
        "id": "ce9295b2-182b-490b-8325-83a67c4a001d"
      },
      "source": [
        "# Coding an LLM architecture\n",
        "## This notebook is inspired from Sebastian Raschka's book: https://github.com/rasbt/LLMs-from-scratch/tree/main/setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "f9eac223-a125-40f7-bacc-bd0d890450c7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f9eac223-a125-40f7-bacc-bd0d890450c7",
        "outputId": "f69961d3-fb72-440c-a46b-7ecfb85887fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch version: 2.8.0+cu126\n",
            "tiktoken version: 0.11.0\n"
          ]
        }
      ],
      "source": [
        "from importlib.metadata import version\n",
        "print(\"torch version:\", version(\"torch\"))\n",
        "print(\"tiktoken version:\", version(\"tiktoken\"))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('/content/')\n",
        "import supplementary"
      ],
      "metadata": {
        "id": "INkwITBcKyU5"
      },
      "id": "INkwITBcKyU5",
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "ad72d1ff-d82d-4e33-a88e-3c1a8831797b",
      "metadata": {
        "id": "ad72d1ff-d82d-4e33-a88e-3c1a8831797b"
      },
      "source": [
        "\n",
        "- Compared to conventional deep learning models, LLMs are larger, mainly due to their vast number of parameters, not the amount of code\n",
        "- We'll see that many elements are repeated in an LLM's architecture\n",
        "- In this notebook, we consider embedding and model sizes akin to a small GPT-2 model\n",
        "- We'll specifically code the architecture of the smallest GPT-2 model (124 million parameters), as outlined in Radford et al.'s [Language Models are Unsupervised Multitask Learners](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) (note that the initial report lists it as 117M parameters, but this was later corrected in the model weight repository)\n",
        "- The next notebook will show how to load pretrained weights into our implementation, which will be compatible with model sizes of 345, 762, and 1542 million parameters\n",
        "- Models like Llama and others are very similar to this model, since they are all based on the same core concepts\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "21baa14d-24b8-4820-8191-a2808f7fbabc",
      "metadata": {
        "id": "21baa14d-24b8-4820-8191-a2808f7fbabc"
      },
      "source": [
        "- Configuration details for the 124 million parameter GPT-2 model (GPT-2 \"small\") include:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ed66875-1f24-445d-add6-006aae3c5707",
      "metadata": {
        "id": "5ed66875-1f24-445d-add6-006aae3c5707"
      },
      "outputs": [],
      "source": [
        "GPT_CONFIG_124M = {\n",
        "    \"vocab_size\": 50257,    # Vocabulary size\n",
        "    \"context_length\": 1024, # Context length\n",
        "    \"emb_dim\": 768,         # Embedding dimension\n",
        "    \"n_heads\": 12,          # Number of attention heads\n",
        "    \"n_layers\": 12,         # Number of layers\n",
        "    \"drop_rate\": 0.0,       # Dropout rate\n",
        "    \"qkv_bias\": False       # Query-Key-Value bias\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "46618527-15ac-4c32-ad85-6cfea83e006e",
      "metadata": {
        "id": "46618527-15ac-4c32-ad85-6cfea83e006e"
      },
      "source": [
        "\n",
        "# Coding the GPT model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dec7d03d-9ff3-4ca3-ad67-01b67c2f5457",
      "metadata": {
        "id": "dec7d03d-9ff3-4ca3-ad67-01b67c2f5457"
      },
      "source": [
        "\n",
        "- Note that the transformer block is repeated multiple times; in the case of the smallest 124M GPT-2 model, we repeat it 12 times.\n",
        "- The corresponding code implementation, where `cfg[\"n_layers\"] = 12`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c61de39c-d03c-4a32-8b57-f49ac3834857",
      "metadata": {
        "id": "c61de39c-d03c-4a32-8b57-f49ac3834857"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "from supplementary import TransformerBlock, LayerNorm\n",
        "\n",
        "\n",
        "class GPTModel(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
        "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
        "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
        "\n",
        "        self.trf_blocks = nn.Sequential(\n",
        "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
        "\n",
        "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
        "        self.out_head = nn.Linear(\n",
        "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
        "        )\n",
        "\n",
        "    def forward(self, in_idx):\n",
        "        batch_size, seq_len = in_idx.shape\n",
        "        tok_embeds = self.tok_emb(in_idx)\n",
        "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
        "        x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]\n",
        "        x = self.drop_emb(x)\n",
        "        x = self.trf_blocks(x)\n",
        "        x = self.final_norm(x)\n",
        "        logits = self.out_head(x)\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2750270f-c45d-4410-8767-a6adbd05d5c3",
      "metadata": {
        "id": "2750270f-c45d-4410-8767-a6adbd05d5c3"
      },
      "source": [
        "- Using the configuration of the 124M parameter model, we can now instantiate this GPT model with random initial weights as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9bf6abb6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9bf6abb6",
        "outputId": "314bb564-1ed8-46c7-a2a8-4ad34333ed8e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[44387,  5805,   318,   281,  7895,  7243,     0],\n",
            "        [ 1135,   389, 13226,   428,  1398,  3842,     0]])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import tiktoken\n",
        "\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "batch = []\n",
        "\n",
        "txt1 = \"SysML is an exciting topic!\"\n",
        "txt2 = \"We are enjoying this class activity!\"\n",
        "\n",
        "batch.append(torch.tensor(tokenizer.encode(txt1)))\n",
        "batch.append(torch.tensor(tokenizer.encode(txt2)))\n",
        "batch = torch.stack(batch, dim=0)\n",
        "print(batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef94fd9c-4e9d-470d-8f8e-dd23d1bb1f64",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ef94fd9c-4e9d-470d-8f8e-dd23d1bb1f64",
        "outputId": "5ab49781-ed14-4aed-9ff6-6a284eb45977"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input batch:\n",
            " tensor([[44387,  5805,   318,   281,  7895,  7243,     0],\n",
            "        [ 1135,   389, 13226,   428,  1398,  3842,     0]])\n",
            "\n",
            "Output shape: torch.Size([2, 7, 50257])\n",
            "tensor([[[ 0.1617, -0.1764,  0.3950,  ...,  0.6123, -0.2504,  0.0080],\n",
            "         [ 0.9303,  0.6303, -0.1192,  ..., -0.7093, -0.7461,  0.6131],\n",
            "         [ 1.4983, -0.6454,  0.0533,  ...,  0.3796,  0.0106, -0.5612],\n",
            "         ...,\n",
            "         [ 1.2678,  0.5919, -0.4522,  ..., -0.8229,  0.1071, -0.5686],\n",
            "         [-0.5783,  0.2095, -0.1544,  ..., -0.4177,  0.5092, -0.2287],\n",
            "         [ 0.2756,  0.9296,  0.4836,  ..., -0.0557,  0.0341, -0.6004]],\n",
            "\n",
            "        [[ 0.8183, -0.0958, -0.3817,  ...,  0.0393,  0.5270, -0.8108],\n",
            "         [ 0.8268, -0.1798, -0.4543,  ...,  0.0599,  0.7603, -0.0481],\n",
            "         [ 1.1810, -0.0887, -0.1735,  ..., -0.3513,  0.6742, -0.2838],\n",
            "         ...,\n",
            "         [ 0.7055,  0.0464, -0.2255,  ..., -0.7350, -0.6667, -0.4484],\n",
            "         [-0.0459,  0.4305, -0.0806,  ..., -0.1916,  0.8593, -0.9296],\n",
            "         [ 0.3297,  0.7435,  0.4529,  ..., -0.0620,  0.1667, -0.6078]]],\n",
            "       grad_fn=<UnsafeViewBackward0>)\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(123)\n",
        "model = GPTModel(GPT_CONFIG_124M)\n",
        "\n",
        "out = model(batch)\n",
        "print(\"Input batch:\\n\", batch)\n",
        "print(\"\\nOutput shape:\", out.shape)\n",
        "print(out)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "44a1bb67-be42-431d-87d0-00c005f4a520",
      "metadata": {
        "id": "44a1bb67-be42-431d-87d0-00c005f4a520"
      },
      "source": [
        "- We will train this model in the next notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da5d9bc0-95ab-45d4-9378-417628d86e35",
      "metadata": {
        "id": "da5d9bc0-95ab-45d4-9378-417628d86e35"
      },
      "source": [
        "\n",
        "# Generating text"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a7061524-a3bd-4803-ade6-2e3b7b79ac13",
      "metadata": {
        "id": "a7061524-a3bd-4803-ade6-2e3b7b79ac13"
      },
      "source": [
        "- LLMs like the GPT model we implemented above are used to generate one word at a time\n",
        "- The following `generate_text_simple` function implements greedy decoding, which is a simple and fast method to generate text\n",
        "- In greedy decoding, at each step, the model chooses the word (or token) with the highest probability as its next output (the highest logit corresponds to the highest probability, so we technically wouldn't even have to compute the softmax function explicitly)\n",
        "- The figure below depicts how the GPT model, given an input context, generates the next word token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9b428a9-8764-4b36-80cd-7d4e00595ba6",
      "metadata": {
        "id": "c9b428a9-8764-4b36-80cd-7d4e00595ba6"
      },
      "outputs": [],
      "source": [
        "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
        "    # idx is (batch, n_tokens) array of indices in the current context\n",
        "    for _ in range(max_new_tokens):\n",
        "\n",
        "        # Crop current context if it exceeds the supported context size\n",
        "        # E.g., if LLM supports only 5 tokens, and the context size is 10\n",
        "        # then only the last 5 tokens are used as context\n",
        "        idx_cond = idx[:, -context_size:]\n",
        "\n",
        "        # Get the predictions\n",
        "        with torch.no_grad():\n",
        "            logits = model(idx_cond)\n",
        "\n",
        "        # Focus only on the last time step\n",
        "        # (batch, n_tokens, vocab_size) becomes (batch, vocab_size)\n",
        "        logits = logits[:, -1, :]\n",
        "\n",
        "        # Apply softmax to get probabilities\n",
        "        probas = torch.softmax(logits, dim=-1)  # (batch, vocab_size)\n",
        "\n",
        "        # Get the idx of the vocab entry with the highest probability value\n",
        "        idx_next = torch.argmax(probas, dim=-1, keepdim=True)  # (batch, 1)\n",
        "\n",
        "        # Append sampled index to the running sequence\n",
        "        idx = torch.cat((idx, idx_next), dim=1)  # (batch, n_tokens+1)\n",
        "\n",
        "    return idx"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6515f2c1-3cc7-421c-8d58-cc2f563b7030",
      "metadata": {
        "id": "6515f2c1-3cc7-421c-8d58-cc2f563b7030"
      },
      "source": [
        "- The `generate_text_simple` above implements an iterative process, where it creates one token at a time\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b0fa8b2c-4d97-4259-a8da-8ffb6bb088be",
      "metadata": {
        "id": "b0fa8b2c-4d97-4259-a8da-8ffb6bb088be"
      },
      "source": [
        "\n",
        "# Exercise: Generate some text"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f682eac4-f9bd-438b-9dec-6b1cc7bc05ce",
      "metadata": {
        "id": "f682eac4-f9bd-438b-9dec-6b1cc7bc05ce"
      },
      "source": [
        "1. Use the `tokenizer.encode` method to prepare some input text\n",
        "2. Then, convert this text into a pytprch tensor via (`torch.tensor`)\n",
        "3. Add a batch dimension via `.unsqueeze(0)`\n",
        "4. Use the `generate_text_simple` function to have the GPT generate some text based on your prepared input text\n",
        "5. The output from step 4 will be token IDs, convert them back into text via the `tokenizer.decode` method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2286f6de-5222-46f8-ad0d-d1f380a36636",
      "metadata": {
        "id": "2286f6de-5222-46f8-ad0d-d1f380a36636"
      },
      "outputs": [],
      "source": [
        "model.eval();  # disable dropout"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "02fa7ae0-f30d-454c-a92a-a75894ea68d2",
      "metadata": {
        "id": "02fa7ae0-f30d-454c-a92a-a75894ea68d2"
      },
      "source": [
        "\n",
        "\n",
        "# Solution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fdc23e58-dd3f-48d8-a767-944c1b6e030f",
      "metadata": {
        "id": "fdc23e58-dd3f-48d8-a767-944c1b6e030f",
        "outputId": "9296f653-4e0a-4367-993e-198206aa54cc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "encoded: [15496, 11, 314, 716]\n",
            "encoded_tensor.shape: torch.Size([1, 4])\n"
          ]
        }
      ],
      "source": [
        "start_context = \"Hello, I am\"\n",
        "\n",
        "encoded = tokenizer.encode(start_context)\n",
        "print(\"encoded:\", encoded)\n",
        "\n",
        "encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
        "print(\"encoded_tensor.shape:\", encoded_tensor.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "599b0821-9755-4cf1-8da4-a1c0fec448b1",
      "metadata": {
        "id": "599b0821-9755-4cf1-8da4-a1c0fec448b1",
        "outputId": "badd3f6c-a441-4623-9c00-739bf91df64a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: tensor([[15496,    11,   314,   716, 27018, 24086, 47843, 30961, 42348,  7267]])\n",
            "Output length: 10\n"
          ]
        }
      ],
      "source": [
        "out = generate_text_simple(\n",
        "    model=model,\n",
        "    idx=encoded_tensor,\n",
        "    max_new_tokens=6,\n",
        "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
        ")\n",
        "\n",
        "print(\"Output:\", out)\n",
        "print(\"Output length:\", len(out[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6de4f875-f967-4089-8410-b5cd2c200de8",
      "metadata": {
        "id": "6de4f875-f967-4089-8410-b5cd2c200de8"
      },
      "source": [
        "- Remove batch dimension and convert back into text:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "74c8d848-8ac1-41d4-b229-72ba7698297c",
      "metadata": {
        "id": "74c8d848-8ac1-41d4-b229-72ba7698297c",
        "outputId": "00817260-8d52-42b1-d0c3-277c0b71b98b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello, I am Featureiman Byeswickattribute argue\n"
          ]
        }
      ],
      "source": [
        "decoded_text = tokenizer.decode(out.squeeze(0).tolist())\n",
        "print(decoded_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8c538bcd-a209-4273-9527-60d6fef1f6ab",
      "metadata": {
        "id": "8c538bcd-a209-4273-9527-60d6fef1f6ab"
      },
      "source": [
        "- Note that the model is untrained; hence the random output texts above\n",
        "- We will train the model in the next notebook"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}