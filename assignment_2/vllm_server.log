2025-09-30 10:57:28,364 - vllm.platforms - INFO - Automatically detected platform cuda.
2025-09-30 10:57:31,559 - vllm.plugins - INFO - Available plugins for group vllm.general_plugins:
2025-09-30 10:57:31,559 - vllm.plugins - INFO - - lora_filesystem_resolver -> vllm.plugins.lora_resolvers.filesystem_resolver:register_filesystem_resolver
2025-09-30 10:57:31,559 - vllm.plugins - INFO - All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2025-09-30 10:57:32,220 - vllm.entrypoints.openai.api_server - INFO - vLLM API server version 0.9.0.2.dev0+g5fbbfe9a4.d20250930
2025-09-30 10:57:32,911 - vllm.entrypoints.openai.cli_args - INFO - non-default args: {'model': 'meta-llama/CodeLlama-34b-hf', 'max_model_len': 16368, 'enforce_eager': True, 'disable_sliding_window': True, 'load_format': 'dummy', 'tensor_parallel_size': 2, 'swap_space': 16.0, 'max_num_seqs': 512, 'preemption_mode': 'swap', 'disable_log_requests': True}
2025-09-30 10:57:38,600 - vllm.platforms - INFO - Automatically detected platform cuda.
2025-09-30 10:57:41,174 - vllm.plugins - INFO - Available plugins for group vllm.general_plugins:
2025-09-30 10:57:41,174 - vllm.plugins - INFO - - lora_filesystem_resolver -> vllm.plugins.lora_resolvers.filesystem_resolver:register_filesystem_resolver
2025-09-30 10:57:41,174 - vllm.plugins - INFO - All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2025-09-30 10:57:43,180 - vllm.config - INFO - This model supports multiple tasks: {'embed', 'reward', 'classify', 'generate', 'score'}. Defaulting to 'generate'.
2025-09-30 10:57:43,227 - vllm.config - INFO - Defaulting to use mp for distributed inference
2025-09-30 10:57:43,227 - vllm.platforms.cuda - WARNING - To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
2025-09-30 10:57:43,236 - vllm.entrypoints.openai.api_server - INFO - Started engine process with PID 844511
2025-09-30 10:57:47,611 - vllm.platforms - INFO - Automatically detected platform cuda.
2025-09-30 10:57:50,360 - vllm.plugins - INFO - Available plugins for group vllm.general_plugins:
2025-09-30 10:57:50,361 - vllm.plugins - INFO - - lora_filesystem_resolver -> vllm.plugins.lora_resolvers.filesystem_resolver:register_filesystem_resolver
2025-09-30 10:57:50,361 - vllm.plugins - INFO - All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2025-09-30 10:57:50,369 - vllm.engine.llm_engine - INFO - Initializing a V0 LLM engine (v0.9.0.2.dev0+g5fbbfe9a4.d20250930) with config: model='meta-llama/CodeLlama-34b-hf', speculative_config=None, tokenizer='meta-llama/CodeLlama-34b-hf', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=16368, download_dir=None, load_format=dummy, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='xgrammar', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=None, served_model_name=meta-llama/CodeLlama-34b-hf, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=False, pooler_config=None, compilation_config={"compile_sizes": [], "cudagraph_capture_sizes": [], "max_capture_size": 0}, use_cached_outputs=True, 
2025-09-30 10:57:50,943 - vllm.executor.multiproc_worker_utils - WARNING - Reducing Torch parallelism from 64 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
2025-09-30 10:57:51,924 - vllm.platforms.cuda - INFO - Using Flash Attention backend.
2025-09-30 10:57:55,228 - vllm.platforms - INFO - Automatically detected platform cuda.
2025-09-30 10:57:57,947 - vllm.executor.multiproc_worker_utils - INFO - Worker ready; awaiting tasks
2025-09-30 10:57:57,960 - vllm.plugins - INFO - Available plugins for group vllm.general_plugins:
2025-09-30 10:57:57,960 - vllm.plugins - INFO - - lora_filesystem_resolver -> vllm.plugins.lora_resolvers.filesystem_resolver:register_filesystem_resolver
2025-09-30 10:57:57,960 - vllm.plugins - INFO - All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2025-09-30 10:57:58,824 - vllm.platforms.cuda - INFO - Using Flash Attention backend.
2025-09-30 10:58:01,031 - vllm.utils - INFO - Found nccl from library libnccl.so.2
2025-09-30 10:58:01,031 - vllm.distributed.device_communicators.pynccl - INFO - vLLM is using nccl==2.21.5
2025-09-30 10:58:01,033 - vllm.utils - INFO - Found nccl from library libnccl.so.2
2025-09-30 10:58:01,034 - vllm.distributed.device_communicators.pynccl - INFO - vLLM is using nccl==2.21.5
2025-09-30 10:58:01,607 - vllm.distributed.device_communicators.custom_all_reduce_utils - INFO - generating GPU P2P access cache in /global/homes/m/mh2653/.cache/vllm/gpu_p2p_access_cache_for_0,1.json
2025-09-30 10:58:05,544 - vllm.platforms - INFO - Automatically detected platform cuda.
2025-09-30 10:58:11,679 - vllm.platforms - INFO - Automatically detected platform cuda.
2025-09-30 10:58:11,699 - vllm.platforms - INFO - Automatically detected platform cuda.
2025-09-30 10:58:18,620 - vllm.distributed.device_communicators.custom_all_reduce_utils - INFO - reading GPU P2P access cache from /global/homes/m/mh2653/.cache/vllm/gpu_p2p_access_cache_for_0,1.json
2025-09-30 10:58:18,620 - vllm.distributed.device_communicators.custom_all_reduce_utils - INFO - reading GPU P2P access cache from /global/homes/m/mh2653/.cache/vllm/gpu_p2p_access_cache_for_0,1.json
2025-09-30 10:58:18,643 - vllm.distributed.device_communicators.shm_broadcast - INFO - vLLM message queue communication handle: Handle(local_reader_ranks=[1], buffer_handle=(1, 4194304, 6, 'psm_56f3c64a'), local_subscribe_addr='ipc:///tmp/aa21c758-9b0d-4411-a1ce-57d7b5529da0', remote_subscribe_addr=None, remote_addr_ipv6=False)
2025-09-30 10:58:18,692 - vllm.distributed.parallel_state - INFO - rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
2025-09-30 10:58:18,693 - vllm.distributed.parallel_state - INFO - rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1
2025-09-30 10:58:18,693 - vllm.worker.model_runner - INFO - Starting to load model meta-llama/CodeLlama-34b-hf...
2025-09-30 10:58:18,694 - vllm.worker.model_runner - INFO - Starting to load model meta-llama/CodeLlama-34b-hf...
2025-09-30 10:58:19,056 - vllm.worker.model_runner - INFO - Model loading took 31.4313 GiB and 0.201762 seconds
2025-09-30 10:58:19,091 - vllm.worker.model_runner - INFO - Model loading took 31.4313 GiB and 0.201503 seconds
2025-09-30 10:58:25,837 - vllm.worker.worker - INFO - Memory profiling takes 6.61 seconds
the current vLLM instance can use total_gpu_memory (79.14GiB) x gpu_memory_utilization (0.90) = 71.22GiB
model weights take 31.43GiB; non_torch_memory takes 0.75GiB; PyTorch activation peak memory takes 1.77GiB; the rest of the memory reserved for KV Cache is 37.28GiB.
2025-09-30 10:58:25,934 - vllm.worker.worker - INFO - Memory profiling takes 6.70 seconds
the current vLLM instance can use total_gpu_memory (79.14GiB) x gpu_memory_utilization (0.90) = 71.22GiB
model weights take 31.43GiB; non_torch_memory takes 0.75GiB; PyTorch activation peak memory takes 1.77GiB; the rest of the memory reserved for KV Cache is 37.28GiB.
2025-09-30 10:58:26,090 - vllm.executor.executor_base - INFO - # cuda blocks: 25450, # CPU blocks: 10922
2025-09-30 10:58:26,090 - vllm.executor.executor_base - INFO - Maximum concurrency for 16368 tokens per request: 24.88x
2025-09-30 10:58:40,660 - vllm.engine.llm_engine - INFO - init engine (profile, create kv cache, warmup model) took 21.57 seconds
2025-09-30 10:58:41,618 - vllm.entrypoints.openai.api_server - INFO - Starting vLLM API server on http://0.0.0.0:8000
2025-09-30 10:58:41,618 - vllm.entrypoints.launcher - INFO - Available routes are:
2025-09-30 10:58:41,618 - vllm.entrypoints.launcher - INFO - Route: /openapi.json, Methods: GET, HEAD
2025-09-30 10:58:41,618 - vllm.entrypoints.launcher - INFO - Route: /docs, Methods: GET, HEAD
2025-09-30 10:58:41,618 - vllm.entrypoints.launcher - INFO - Route: /docs/oauth2-redirect, Methods: GET, HEAD
2025-09-30 10:58:41,618 - vllm.entrypoints.launcher - INFO - Route: /redoc, Methods: GET, HEAD
2025-09-30 10:58:41,619 - vllm.entrypoints.launcher - INFO - Route: /health, Methods: GET
2025-09-30 10:58:41,619 - vllm.entrypoints.launcher - INFO - Route: /load, Methods: GET
2025-09-30 10:58:41,619 - vllm.entrypoints.launcher - INFO - Route: /ping, Methods: POST
2025-09-30 10:58:41,619 - vllm.entrypoints.launcher - INFO - Route: /ping, Methods: GET
2025-09-30 10:58:41,619 - vllm.entrypoints.launcher - INFO - Route: /tokenize, Methods: POST
2025-09-30 10:58:41,619 - vllm.entrypoints.launcher - INFO - Route: /detokenize, Methods: POST
2025-09-30 10:58:41,619 - vllm.entrypoints.launcher - INFO - Route: /v1/models, Methods: GET
2025-09-30 10:58:41,619 - vllm.entrypoints.launcher - INFO - Route: /version, Methods: GET
2025-09-30 10:58:41,619 - vllm.entrypoints.launcher - INFO - Route: /v1/chat/completions, Methods: POST
2025-09-30 10:58:41,619 - vllm.entrypoints.launcher - INFO - Route: /v1/completions, Methods: POST
2025-09-30 10:58:41,620 - vllm.entrypoints.launcher - INFO - Route: /v1/embeddings, Methods: POST
2025-09-30 10:58:41,620 - vllm.entrypoints.launcher - INFO - Route: /pooling, Methods: POST
2025-09-30 10:58:41,620 - vllm.entrypoints.launcher - INFO - Route: /classify, Methods: POST
2025-09-30 10:58:41,620 - vllm.entrypoints.launcher - INFO - Route: /score, Methods: POST
2025-09-30 10:58:41,620 - vllm.entrypoints.launcher - INFO - Route: /v1/score, Methods: POST
2025-09-30 10:58:41,620 - vllm.entrypoints.launcher - INFO - Route: /v1/audio/transcriptions, Methods: POST
2025-09-30 10:58:41,620 - vllm.entrypoints.launcher - INFO - Route: /rerank, Methods: POST
2025-09-30 10:58:41,620 - vllm.entrypoints.launcher - INFO - Route: /v1/rerank, Methods: POST
2025-09-30 10:58:41,620 - vllm.entrypoints.launcher - INFO - Route: /v2/rerank, Methods: POST
2025-09-30 10:58:41,620 - vllm.entrypoints.launcher - INFO - Route: /invocations, Methods: POST
2025-09-30 10:58:41,620 - vllm.entrypoints.launcher - INFO - Route: /metrics, Methods: GET
2025-09-30 10:59:46,448 - vllm.engine.metrics - INFO - Avg prompt throughput: 253.9 tokens/s, Avg generation throughput: 5.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
2025-09-30 10:59:51,449 - vllm.engine.metrics - INFO - Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 37.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.
2025-09-30 10:59:56,560 - vllm.engine.metrics - INFO - Avg prompt throughput: 3882.9 tokens/s, Avg generation throughput: 62.6 tokens/s, Running: 11 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.0%, CPU KV cache usage: 0.0%.
2025-09-30 11:00:01,562 - vllm.engine.metrics - INFO - Avg prompt throughput: 2782.2 tokens/s, Avg generation throughput: 267.7 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 8.1%, CPU KV cache usage: 0.0%.
2025-09-30 11:00:06,730 - vllm.engine.metrics - INFO - Avg prompt throughput: 3998.4 tokens/s, Avg generation throughput: 204.5 tokens/s, Running: 34 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.8%, CPU KV cache usage: 0.0%.
2025-09-30 11:00:11,754 - vllm.engine.metrics - INFO - Avg prompt throughput: 3258.6 tokens/s, Avg generation throughput: 467.8 tokens/s, Running: 37 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 14.2%, CPU KV cache usage: 0.0%.
2025-09-30 11:00:16,774 - vllm.engine.metrics - INFO - Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 860.5 tokens/s, Running: 25 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 11.3%, CPU KV cache usage: 0.0%.
2025-09-30 11:00:21,783 - vllm.engine.metrics - INFO - Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 577.3 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.6%, CPU KV cache usage: 0.0%.
2025-09-30 11:00:26,783 - vllm.engine.metrics - INFO - Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 346.2 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.9%, CPU KV cache usage: 0.0%.
2025-09-30 11:00:31,793 - vllm.engine.metrics - INFO - Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 130.3 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.4%, CPU KV cache usage: 0.0%.
2025-09-30 11:00:36,809 - vllm.engine.metrics - INFO - Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 93.9 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.3%, CPU KV cache usage: 0.0%.
2025-09-30 11:00:49,144 - vllm.engine.metrics - INFO - Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 12.1 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
2025-09-30 11:00:59,155 - vllm.engine.metrics - INFO - Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
2025-09-30 11:10:14,680 - vllm.engine.metrics - INFO - Avg prompt throughput: 255.1 tokens/s, Avg generation throughput: 15.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.
2025-09-30 11:10:19,685 - vllm.engine.metrics - INFO - Avg prompt throughput: 650.7 tokens/s, Avg generation throughput: 33.8 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.8%, CPU KV cache usage: 0.0%.
2025-09-30 11:10:24,693 - vllm.engine.metrics - INFO - Avg prompt throughput: 3582.5 tokens/s, Avg generation throughput: 136.4 tokens/s, Running: 13 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.1%, CPU KV cache usage: 0.0%.
2025-09-30 11:10:29,706 - vllm.engine.metrics - INFO - Avg prompt throughput: 3090.7 tokens/s, Avg generation throughput: 274.7 tokens/s, Running: 22 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 8.5%, CPU KV cache usage: 0.0%.
2025-09-30 11:10:34,778 - vllm.engine.metrics - INFO - Avg prompt throughput: 3750.3 tokens/s, Avg generation throughput: 291.8 tokens/s, Running: 34 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 12.6%, CPU KV cache usage: 0.0%.
2025-09-30 11:10:39,790 - vllm.engine.metrics - INFO - Avg prompt throughput: 3009.7 tokens/s, Avg generation throughput: 527.0 tokens/s, Running: 35 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 14.1%, CPU KV cache usage: 0.0%.
2025-09-30 11:10:44,804 - vllm.engine.metrics - INFO - Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 792.2 tokens/s, Running: 23 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 10.2%, CPU KV cache usage: 0.0%.
2025-09-30 11:10:49,809 - vllm.engine.metrics - INFO - Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 522.0 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.7%, CPU KV cache usage: 0.0%.
2025-09-30 11:10:54,827 - vllm.engine.metrics - INFO - Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 281.4 tokens/s, Running: 4 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%.
2025-09-30 11:10:59,848 - vllm.engine.metrics - INFO - Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 115.7 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.4%, CPU KV cache usage: 0.0%.
2025-09-30 11:11:04,862 - vllm.engine.metrics - INFO - Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 85.4 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.3%, CPU KV cache usage: 0.0%.
2025-09-30 11:11:16,016 - vllm.engine.metrics - INFO - Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 5.6 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
2025-09-30 11:11:26,027 - vllm.engine.metrics - INFO - Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
