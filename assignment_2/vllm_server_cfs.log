2025-10-04 08:08:50,131 - vllm.entrypoints.launcher - INFO - Shutting down FastAPI HTTP server.
2025-10-04 11:25:27,920 - vllm.platforms - INFO - Automatically detected platform cuda.
2025-10-04 11:25:42,557 - vllm.platforms - INFO - Automatically detected platform cuda.
2025-10-04 11:25:46,147 - vllm.plugins - INFO - Available plugins for group vllm.general_plugins:
2025-10-04 11:25:46,147 - vllm.plugins - INFO - - lora_filesystem_resolver -> vllm.plugins.lora_resolvers.filesystem_resolver:register_filesystem_resolver
2025-10-04 11:25:46,147 - vllm.plugins - INFO - All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2025-10-04 11:25:46,862 - vllm.entrypoints.openai.api_server - INFO - vLLM API server version 0.9.0.2.dev0+g5fbbfe9a4.d20250930
2025-10-04 11:25:47,423 - vllm.entrypoints.openai.cli_args - INFO - non-default args: {'model': 'meta-llama/CodeLlama-34b-hf', 'max_model_len': 16368, 'enforce_eager': True, 'disable_sliding_window': True, 'load_format': 'dummy', 'tensor_parallel_size': 2, 'swap_space': 16.0, 'max_num_seqs': 512, 'preemption_mode': 'swap', 'disable_log_requests': True}
2025-10-04 11:25:52,837 - vllm.platforms - INFO - Automatically detected platform cuda.
2025-10-04 11:25:55,336 - vllm.plugins - INFO - Available plugins for group vllm.general_plugins:
2025-10-04 11:25:55,336 - vllm.plugins - INFO - - lora_filesystem_resolver -> vllm.plugins.lora_resolvers.filesystem_resolver:register_filesystem_resolver
2025-10-04 11:25:55,336 - vllm.plugins - INFO - All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2025-10-04 11:25:57,618 - vllm.config - INFO - This model supports multiple tasks: {'embed', 'classify', 'score', 'reward', 'generate'}. Defaulting to 'generate'.
2025-10-04 11:25:57,658 - vllm.config - INFO - Defaulting to use mp for distributed inference
2025-10-04 11:25:57,659 - vllm.platforms.cuda - WARNING - To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
2025-10-04 11:25:57,673 - vllm.entrypoints.openai.api_server - INFO - Started engine process with PID 138863
2025-10-04 11:26:02,222 - vllm.platforms - INFO - Automatically detected platform cuda.
2025-10-04 11:26:04,939 - vllm.plugins - INFO - Available plugins for group vllm.general_plugins:
2025-10-04 11:26:04,940 - vllm.plugins - INFO - - lora_filesystem_resolver -> vllm.plugins.lora_resolvers.filesystem_resolver:register_filesystem_resolver
2025-10-04 11:26:04,940 - vllm.plugins - INFO - All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2025-10-04 11:26:04,945 - vllm.engine.llm_engine - INFO - Initializing a V0 LLM engine (v0.9.0.2.dev0+g5fbbfe9a4.d20250930) with config: model='meta-llama/CodeLlama-34b-hf', speculative_config=None, tokenizer='meta-llama/CodeLlama-34b-hf', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=16368, download_dir=None, load_format=dummy, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='xgrammar', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=None, served_model_name=meta-llama/CodeLlama-34b-hf, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=False, pooler_config=None, compilation_config={"compile_sizes": [], "cudagraph_capture_sizes": [], "max_capture_size": 0}, use_cached_outputs=True, 
2025-10-04 11:26:05,568 - vllm.executor.multiproc_worker_utils - WARNING - Reducing Torch parallelism from 64 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
2025-10-04 11:26:06,563 - vllm.platforms.cuda - INFO - Using Flash Attention backend.
2025-10-04 11:26:09,885 - vllm.platforms - INFO - Automatically detected platform cuda.
2025-10-04 11:26:12,538 - vllm.executor.multiproc_worker_utils - INFO - Worker ready; awaiting tasks
2025-10-04 11:26:12,551 - vllm.plugins - INFO - Available plugins for group vllm.general_plugins:
2025-10-04 11:26:12,551 - vllm.plugins - INFO - - lora_filesystem_resolver -> vllm.plugins.lora_resolvers.filesystem_resolver:register_filesystem_resolver
2025-10-04 11:26:12,551 - vllm.plugins - INFO - All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2025-10-04 11:26:13,421 - vllm.platforms.cuda - INFO - Using Flash Attention backend.
2025-10-04 11:26:15,212 - vllm.utils - INFO - Found nccl from library libnccl.so.2
2025-10-04 11:26:15,212 - vllm.utils - INFO - Found nccl from library libnccl.so.2
2025-10-04 11:26:15,212 - vllm.distributed.device_communicators.pynccl - INFO - vLLM is using nccl==2.21.5
2025-10-04 11:26:15,212 - vllm.distributed.device_communicators.pynccl - INFO - vLLM is using nccl==2.21.5
2025-10-04 11:26:15,769 - vllm.distributed.device_communicators.custom_all_reduce_utils - INFO - reading GPU P2P access cache from /global/homes/m/mh2653/.cache/vllm/gpu_p2p_access_cache_for_0,1.json
2025-10-04 11:26:15,769 - vllm.distributed.device_communicators.custom_all_reduce_utils - INFO - reading GPU P2P access cache from /global/homes/m/mh2653/.cache/vllm/gpu_p2p_access_cache_for_0,1.json
2025-10-04 11:26:15,804 - vllm.distributed.device_communicators.shm_broadcast - INFO - vLLM message queue communication handle: Handle(local_reader_ranks=[1], buffer_handle=(1, 4194304, 6, 'psm_4e803f15'), local_subscribe_addr='ipc:///tmp/09f54748-d53e-47f1-b902-4a6e04592b3f', remote_subscribe_addr=None, remote_addr_ipv6=False)
2025-10-04 11:26:15,836 - vllm.distributed.parallel_state - INFO - rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
2025-10-04 11:26:15,836 - vllm.distributed.parallel_state - INFO - rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1
2025-10-04 11:26:15,836 - vllm.worker.model_runner - INFO - Starting to load model meta-llama/CodeLlama-34b-hf...
2025-10-04 11:26:15,837 - vllm.worker.model_runner - INFO - Starting to load model meta-llama/CodeLlama-34b-hf...
2025-10-04 11:26:16,199 - vllm.worker.model_runner - INFO - Model loading took 31.4313 GiB and 0.205971 seconds
2025-10-04 11:26:16,235 - vllm.worker.model_runner - INFO - Model loading took 31.4313 GiB and 0.205839 seconds
2025-10-04 11:26:23,080 - vllm.worker.worker - INFO - Memory profiling takes 6.66 seconds
the current vLLM instance can use total_gpu_memory (39.38GiB) x gpu_memory_utilization (0.90) = 35.44GiB
model weights take 31.43GiB; non_torch_memory takes 0.75GiB; PyTorch activation peak memory takes 1.77GiB; the rest of the memory reserved for KV Cache is 1.50GiB.
2025-10-04 11:26:23,161 - vllm.worker.worker - INFO - Memory profiling takes 6.75 seconds
the current vLLM instance can use total_gpu_memory (39.38GiB) x gpu_memory_utilization (0.90) = 35.44GiB
model weights take 31.43GiB; non_torch_memory takes 0.75GiB; PyTorch activation peak memory takes 1.77GiB; the rest of the memory reserved for KV Cache is 1.50GiB.
2025-10-04 11:26:23,318 - vllm.executor.executor_base - INFO - # cuda blocks: 1023, # CPU blocks: 10922
2025-10-04 11:26:23,318 - vllm.executor.executor_base - INFO - Maximum concurrency for 16368 tokens per request: 1.00x
2025-10-04 11:26:38,487 - vllm.engine.llm_engine - INFO - init engine (profile, create kv cache, warmup model) took 22.25 seconds
2025-10-04 11:26:38,899 - vllm.entrypoints.openai.api_server - INFO - Starting vLLM API server on http://0.0.0.0:8000
2025-10-04 11:26:38,899 - vllm.entrypoints.launcher - INFO - Available routes are:
2025-10-04 11:26:38,899 - vllm.entrypoints.launcher - INFO - Route: /openapi.json, Methods: GET, HEAD
2025-10-04 11:26:38,899 - vllm.entrypoints.launcher - INFO - Route: /docs, Methods: GET, HEAD
2025-10-04 11:26:38,899 - vllm.entrypoints.launcher - INFO - Route: /docs/oauth2-redirect, Methods: GET, HEAD
2025-10-04 11:26:38,899 - vllm.entrypoints.launcher - INFO - Route: /redoc, Methods: GET, HEAD
2025-10-04 11:26:38,900 - vllm.entrypoints.launcher - INFO - Route: /health, Methods: GET
2025-10-04 11:26:38,900 - vllm.entrypoints.launcher - INFO - Route: /load, Methods: GET
2025-10-04 11:26:38,900 - vllm.entrypoints.launcher - INFO - Route: /ping, Methods: POST
2025-10-04 11:26:38,900 - vllm.entrypoints.launcher - INFO - Route: /ping, Methods: GET
2025-10-04 11:26:38,900 - vllm.entrypoints.launcher - INFO - Route: /tokenize, Methods: POST
2025-10-04 11:26:38,900 - vllm.entrypoints.launcher - INFO - Route: /detokenize, Methods: POST
2025-10-04 11:26:38,900 - vllm.entrypoints.launcher - INFO - Route: /v1/models, Methods: GET
2025-10-04 11:26:38,900 - vllm.entrypoints.launcher - INFO - Route: /version, Methods: GET
2025-10-04 11:26:38,900 - vllm.entrypoints.launcher - INFO - Route: /v1/chat/completions, Methods: POST
2025-10-04 11:26:38,900 - vllm.entrypoints.launcher - INFO - Route: /v1/completions, Methods: POST
2025-10-04 11:26:38,900 - vllm.entrypoints.launcher - INFO - Route: /v1/embeddings, Methods: POST
2025-10-04 11:26:38,901 - vllm.entrypoints.launcher - INFO - Route: /pooling, Methods: POST
2025-10-04 11:26:38,901 - vllm.entrypoints.launcher - INFO - Route: /classify, Methods: POST
2025-10-04 11:26:38,901 - vllm.entrypoints.launcher - INFO - Route: /score, Methods: POST
2025-10-04 11:26:38,901 - vllm.entrypoints.launcher - INFO - Route: /v1/score, Methods: POST
2025-10-04 11:26:38,901 - vllm.entrypoints.launcher - INFO - Route: /v1/audio/transcriptions, Methods: POST
2025-10-04 11:26:38,901 - vllm.entrypoints.launcher - INFO - Route: /rerank, Methods: POST
2025-10-04 11:26:38,901 - vllm.entrypoints.launcher - INFO - Route: /v1/rerank, Methods: POST
2025-10-04 11:26:38,901 - vllm.entrypoints.launcher - INFO - Route: /v2/rerank, Methods: POST
2025-10-04 11:26:38,901 - vllm.entrypoints.launcher - INFO - Route: /invocations, Methods: POST
2025-10-04 11:26:38,901 - vllm.entrypoints.launcher - INFO - Route: /metrics, Methods: GET
2025-10-04 11:27:17,206 - vllm.engine.metrics - INFO - Avg prompt throughput: 149.7 tokens/s, Avg generation throughput: 0.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.8%, CPU KV cache usage: 0.0%.
2025-10-04 11:27:22,222 - vllm.engine.metrics - INFO - Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 24.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 8.6%, CPU KV cache usage: 0.0%.
2025-10-04 11:27:27,482 - vllm.engine.metrics - INFO - Avg prompt throughput: 1497.1 tokens/s, Avg generation throughput: 25.5 tokens/s, Running: 4 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 48.5%, CPU KV cache usage: 0.0%.
2025-10-04 11:27:28,777 - vllm.core.scheduler - WARNING - Sequence group cmpl-d7bca96fcca347a88a90d02ecd2fc167-0 is preempted by PreemptionMode.SWAP mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1
2025-10-04 11:27:32,696 - vllm.engine.metrics - INFO - Avg prompt throughput: 3294.4 tokens/s, Avg generation throughput: 103.2 tokens/s, Running: 11 reqs, Swapped: 5 reqs, Pending: 2 reqs, GPU KV cache usage: 95.4%, CPU KV cache usage: 5.8%.
2025-10-04 11:27:37,988 - vllm.engine.metrics - INFO - Avg prompt throughput: 2737.4 tokens/s, Avg generation throughput: 157.6 tokens/s, Running: 12 reqs, Swapped: 15 reqs, Pending: 0 reqs, GPU KV cache usage: 91.2%, CPU KV cache usage: 14.2%.
2025-10-04 11:27:43,419 - vllm.engine.metrics - INFO - Avg prompt throughput: 2816.8 tokens/s, Avg generation throughput: 157.8 tokens/s, Running: 11 reqs, Swapped: 24 reqs, Pending: 7 reqs, GPU KV cache usage: 96.1%, CPU KV cache usage: 20.9%.
2025-10-04 11:27:48,437 - vllm.engine.metrics - INFO - Avg prompt throughput: 3181.0 tokens/s, Avg generation throughput: 120.6 tokens/s, Running: 10 reqs, Swapped: 32 reqs, Pending: 0 reqs, GPU KV cache usage: 96.6%, CPU KV cache usage: 28.3%.
2025-10-04 11:27:53,133 - vllm.core.scheduler - WARNING - Sequence group cmpl-b77aa33169974c1e9fedf17ce2f26dc3-0 is preempted by PreemptionMode.SWAP mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=51
2025-10-04 11:27:53,462 - vllm.engine.metrics - INFO - Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 207.8 tokens/s, Running: 8 reqs, Swapped: 31 reqs, Pending: 0 reqs, GPU KV cache usage: 95.5%, CPU KV cache usage: 27.1%.
2025-10-04 11:27:58,480 - vllm.engine.metrics - INFO - Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 187.3 tokens/s, Running: 9 reqs, Swapped: 29 reqs, Pending: 0 reqs, GPU KV cache usage: 91.9%, CPU KV cache usage: 27.9%.
2025-10-04 11:28:03,497 - vllm.engine.metrics - INFO - Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 263.5 tokens/s, Running: 11 reqs, Swapped: 24 reqs, Pending: 0 reqs, GPU KV cache usage: 92.0%, CPU KV cache usage: 25.8%.
2025-10-04 11:28:08,527 - vllm.engine.metrics - INFO - Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 204.0 tokens/s, Running: 8 reqs, Swapped: 25 reqs, Pending: 0 reqs, GPU KV cache usage: 74.1%, CPU KV cache usage: 25.5%.
2025-10-04 11:28:13,619 - vllm.engine.metrics - INFO - Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 184.4 tokens/s, Running: 12 reqs, Swapped: 19 reqs, Pending: 0 reqs, GPU KV cache usage: 98.1%, CPU KV cache usage: 21.3%.
2025-10-04 11:28:13,918 - vllm.core.scheduler - WARNING - Sequence group cmpl-d7bca96fcca347a88a90d02ecd2fc167-0 is preempted by PreemptionMode.SWAP mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=101
2025-10-04 11:28:18,622 - vllm.engine.metrics - INFO - Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 216.3 tokens/s, Running: 6 reqs, Swapped: 21 reqs, Pending: 0 reqs, GPU KV cache usage: 91.2%, CPU KV cache usage: 19.4%.
2025-10-04 11:28:23,644 - vllm.engine.metrics - INFO - Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 182.2 tokens/s, Running: 8 reqs, Swapped: 16 reqs, Pending: 0 reqs, GPU KV cache usage: 94.7%, CPU KV cache usage: 17.1%.
2025-10-04 11:28:28,717 - vllm.engine.metrics - INFO - Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 182.0 tokens/s, Running: 7 reqs, Swapped: 12 reqs, Pending: 0 reqs, GPU KV cache usage: 98.9%, CPU KV cache usage: 13.1%.
2025-10-04 11:28:33,750 - vllm.engine.metrics - INFO - Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 161.5 tokens/s, Running: 7 reqs, Swapped: 10 reqs, Pending: 0 reqs, GPU KV cache usage: 96.5%, CPU KV cache usage: 11.1%.
2025-10-04 11:28:34,910 - vllm.core.scheduler - WARNING - Sequence group cmpl-b267d87ed76d48b28a43addc27a5b254-0 is preempted by PreemptionMode.SWAP mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=151
2025-10-04 11:28:38,780 - vllm.engine.metrics - INFO - Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 174.2 tokens/s, Running: 6 reqs, Swapped: 7 reqs, Pending: 0 reqs, GPU KV cache usage: 84.5%, CPU KV cache usage: 6.9%.
2025-10-04 11:28:43,801 - vllm.engine.metrics - INFO - Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 163.5 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 98.5%, CPU KV cache usage: 0.0%.
2025-10-04 11:28:48,816 - vllm.engine.metrics - INFO - Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 182.0 tokens/s, Running: 4 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 37.6%, CPU KV cache usage: 0.0%.
2025-10-04 11:28:53,841 - vllm.engine.metrics - INFO - Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 96.3 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 36.3%, CPU KV cache usage: 0.0%.

[2025-10-04T11:28:58.366892] Benchmark Summary
============ Serving Benchmark Result ============
Successful requests:                     50
Benchmark duration (s):                  92.40
Total input tokens:                      67785
Total generated tokens:                  17166
Request throughput (req/s):              0.54
Input token throughput (tok/s):          733.57
Output token throughput (tok/s):         185.77
---------------Time to First Token----------------
Mean TTFT (ms):                          1648.87
Median TTFT (ms):                        1169.43
P99 TTFT (ms):                           5073.76
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          138.43
Median TPOT (ms):                        133.61
P99 TPOT (ms):                           276.49
---------------Inter-token Latency----------------
Mean ITL (ms):                           147.74
Median ITL (ms):                         34.24
P99 ITL (ms):                            2647.05
==================================================
2025-10-04 11:29:08,327 - vllm.engine.metrics - INFO - Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 21.2 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
2025-10-04 11:29:18,332 - vllm.engine.metrics - INFO - Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
